# -*- coding: utf-8 -*-
"""Sanket_Shah_Lauretta_Take_Home_Test_ML_L-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-ISLm12G0gfPxweDMQoFglQRsebs0Hp
"""

# Install yt-dlp to download YouTube videos
!pip install yt-dlp

# Download the video (video + audio) from the specified YouTube link
!yt-dlp -f "bv,ba" https://www.youtube.com/watch?v=y2zyucfCyjM

# Rename the downloaded file to a simpler name (bridge_video.mp4)
import os
os.rename("Drone Footage of Canberra's HISTORIC Crowd [y2zyucfCyjM].mp4", "bridge_video.mp4")

# Import necessary libraries for video processing
import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim
from google.colab.patches import cv2_imshow
import tensorflow as tf
import tensorflow_hub as hub
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
import moviepy.editor as mp

# Set up video capture

# Open the video using OpenCV
cap = cv2.VideoCapture('/content/bridge_video.mp4')

# Set start and end times (in seconds) for extracting frames
start_time, end_time = 14, 32
fps = cap.get(cv2.CAP_PROP_FPS)
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Calculate the frame range for the specified time window
start_frame = int(fps * start_time)
end_frame = int(fps * end_time)

# Initialize list to store people count in each processed frame
people_counts = []

# Set the video to start at the specified frame
cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

# Initialize a counter for frames starting from the start frame
frame_counter = start_frame

# Loop through the video until the end frame is reached
while cap.isOpened():
    ret, frame = cap.read() # Read a frame from the video
    if frame_counter > end_frame:
      break # Stop if we've passed the end frame
    if not ret:
      break # Stop if the frame could not be read

    # Process every second (or every fps number of frames)
    if frame_counter % fps == 0:
      # Save the frame as an image file
      cv2.imwrite('img'+str(frame_counter)+'.jpg',frame)
      # Display the current frame
      cv2_imshow(frame)
      print(frame_counter)
      # Exit the loop if the 'q' key is pressed
      if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    # Increment the frame counter
    frame_counter += 1

# Release the video capture and close any OpenCV windows
cap.release()
cv2.destroyAllWindows()

# Import necessary libraries for crowd counting using CSRNet
import h5py
import scipy.io as io
import PIL.Image as Image
import numpy as np
from matplotlib import pyplot as plt, cm as c
from scipy.ndimage.filters import gaussian_filter
import scipy
import torchvision.transforms.functional as F
from model import CSRNet # Import CSRNet model definition
import torch
from torchvision import transforms

# Define the image transformation pipeline for pre-processing
transform=transforms.Compose([
                       transforms.ToTensor()# Convert the image to a Tensor
                       ,transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]),
                   ])

# Normalize using ImageNet std deviations
model = CSRNet()

# Load pre-trained weights into the model
checkpoint = torch.load('weights.pth', map_location="cpu") # Load weights
model.load_state_dict(checkpoint)# Apply the weights to the model

# Loop through the selected frames for crowd counting
i = start_frame
while i in range(start_frame, end_frame + 1):
  print(i)

  img_path = '/content/img'+str(i)+'.jpg' # Get the path to the saved image frame
  print("Original Image") # Display the original image
  plt.imshow(plt.imread(img_path))
  plt.show()

  # Pre-process the image and pass it through the CSRNet model
  img = transform(Image.open(img_path).convert('RGB')) # Convert image to RGB and apply transformations
  output = model(img.unsqueeze(0)) # Add a batch dimension and get the output from the model
  count = int(output.detach().cpu().sum().numpy()) # Get the predicted crowd count
  print("Predicted Count : ", count)

  # Convert the output density map to a NumPy array and display it as a heatmap
  temp = np.asarray(output.detach().cpu().reshape(output.detach().cpu().shape[2],output.detach().cpu().shape[3]))
  plt.imshow(temp,cmap = c.jet)
  plt.show()

  # Append the predicted count to the list
  people_counts.append(count) #Increment frame by 25 (the FPS) to process every 25th frame (approx. one second)
  i += fps

people_counts #Print the people count for each frame

# Sum the max count of every 5 consecutive frames and calculate the total count
i = 0
frame_mean = []
frame_median = []
frame_max = []

while i in range(len(people_counts)):
  # Append the max, mean and median crowd count from every 5-frame window
  frame_mean.append(np.mean(people_counts[i:i+5]))
  frame_median.append(np.median(people_counts[i:i+5]))
  frame_max.append(np.max(people_counts[i:i+5]))
  i+=5

# Calculate the total estimated crowd count
# Store the sum of the mean median and max crowd count from every 5-frame window
result_mean = np.sum(frame_mean)
result_median = np.sum(frame_median)
result_max = np.sum(frame_max)
print("Mean: ", result_mean)
print("Median: ", result_median)
print("Max: ", result_max)

print("Mean: ", frame_mean)
print("Median: ", frame_median)
print("Max: ", frame_max)

# Visualize the maximum counts in specific frames
i = 0
img = 350 # Frame number to start visualization
j = 0

# Loop through and display frames with the corresponding max crowd count
while i in range(len(people_counts)):
  img_path = '/content/img'+str(img)+'.jpg'

  print("Frame No.: ", img)
  print("Predicted Count : ", people_counts[i])
  print("Max Count : ", frame_mean[j])

  # Display the frame and its count information
  plt.imshow(plt.imread(img_path))
  plt.show()

  i+=5
  img+=125
  j+=1

"""Reference for the model used:

https://www.rootstrap.com/blog/how-to-build-a-crowd-counting-model-using-csrnet
"""